
# Measuring and Explaining Creativity in Large Language Models: A Mixed-Methods Investigation into the Effect of Cultural Persona Prompts on Business Solution Generation - Youssef Hariri - Doctoral student in Business of AI - Rennes School of Business and UpGrad
[![DOI](https://img.shields.io/badge/DOI-10.5281/zenodo.17407392-blue)](https://doi.org/10.5281/zenodo.17407392)

This repository contains the data, prompts, and results for the research paper: *"Measuring and Explaining Creativity in Large Language Models: A Mixed-Methods Investigation into the Effect of Cultural Persona Prompts on Business Solution Generation* (soon to be available online).

The files are organized into three categories:
1.  **Input Data:** The datasets used as stimuli for the LLM.
2.  **Prompts:** The evaluation prompts used to score the LLM's output, showing the optimization process.
3.  **Results:** The raw data from the quantitative and qualitative analysis.

---

## ðŸ“œ The Paper

* ** `business_problems_data.json Measuring and Explaining Creativity in LLMs - Hariri Youssef.pdf` **
    
---

## 1. Input Data

* **`business_problems_data.json`**
    * A JSON file containing the set of standardized business problems that were presented to the LLM.
* **`cultural_personas_data.json`**
    * A JSON file defining the "cultural personas" (e.g., Innovator, Pragmatist, Traditionalist) used to prime the LLM.
* **`125 real world examples.txt`**
    * A text file containing the raw 125 real-world business examples that were used to inspire or synthesize the final `business_problems_data.json`.

---

## 2. Prompts (Methodology)

This folder shows the iterative refinement of the prompt used to *evaluate* the creativity of the LLM's solutions.

* **`evaluation_prompt_original.md`**
    * The original, baseline (v0) prompt used for automated evaluation.
* **`evaluation_prompt_optimization_1.md`**
    * The first iteration (v1) of the refined evaluation prompt.
* **`evaluation_prompt_optimization_2.md`**
    * The second and final iteration (v2) of the evaluation prompt used to generate the scores in the paper.

---

---

## 3. Results & Analysis

* **`all_experiment_solutions.json`**
    * A JSON file containing the complete, raw text output of all solutions generated by the LLM for every problem and persona combination. Each entry links a `problem_id`, `persona_id`, and `solution_id` to the full text of the generated solution.

* **`cleaned_evaluation_scores.csv`**
    * This is the main quantitative results file (tracked with Git LFS). It contains the raw, structured scores for all generated solutions, mapping each `solution_id` to its evaluated creativity components: originality, practicality/feasibility, and innovation/novelty.

* **`ensemble_scores_FULL_DATA_6375.csv`**
    * The complete quantitative dataset. This CSV file contains all 6,375 scored solutions generated by the LLM, with scores for creativity, feasibility, novelty, etc., across all personas and problems.

* **`selected_solutions_for_qualitative_study.csv`**
    * A subset of the main dataset, this CSV lists the specific solutions that were selected for the in-depth qualitative (human) analysis.

* **`selected_solutions_justifications_qualitative_study.csv`** (The corrected filename)
    * The results of the qualitative analysis. This file contains the human-provided justifications, coding, and analysis for the solutions listed above.

---

## How to Cite
```bibtex
@software{youssef_hariri_2025_17407392,
  author       = {Hariri, Youssef},
  title        = {Measuring and Explaining Creativity in Large Language Models: A Mixed-Methods Investigation into the Effect of Cultural Persona Prompts on Business Solution Generation},
  month        = oct,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {v1.0.0},
  doi          = {10.5281/zenodo.17407392},
  url          = {https://doi.org/10.5281/zenodo.17407392}
}
```
